{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = \"glove.6B.200d.txt.w2v\"\n",
    "\n",
    "# this takes a while to load -- keep this in mind when designing your capstone project\n",
    "glove = KeyedVectors.load_word2vec_format(get_data_path(filename), binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Image Embedding\n",
    "### 4.1 Image Features\n",
    "\n",
    "To bootstrap our image embedding capability, we're going to make use of a pre-trained computer vision model that was trained to do image classification on the ImageNet dataset. (For more info on ImageNet, see: http://www.image-net.org/).\n",
    "\n",
    "In particular, we're going to use the ResNet-18 model (implemented in PyTorch). ResNet-18 is a type of convolutional neural network (CNN). The last layer of ResNet-18 is a fully connected layer (\"dense\" layer in MyNN terminology; \"Linear\" layer in PyTorch terminology) that projects the final 512-dimensional \"abstract features\" to 1000-dimensional scores used for computing probabilities that an input image is one of the 1000 possible ImageNet classes. We're going to use these 512-dimensional abstract features (which have distilled useful properties/characteristics/aspects of the image) as a starting point for our image embedder.\n",
    "\n",
    "You can imagine that we have a function called `extract_features_resnet18(image)` that takes an image (e.g., in PIL format), runs it through the ResNet-18 model, and then returns a NumPy array of shape (1, 512). You don't have to write this function yourself, though! We've already pre-extracted these 512-dimensional image features for images in the COCO dataset (described below).\n",
    "\n",
    "The file `resnet18_features.pkl` contains a dictionary that maps image ids (from the COCO dataset) to extracted image features, imported below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved image descriptor vectors\n",
    "import pickle\n",
    "with Path(get_data_path('resnet18_features.pkl')).open('rb') as f:\n",
    "    resnet18_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Semantic Embedding\n",
    "\n",
    "We will learn a mapping from the 512-dimensional image feature space to the common 50-dimensional semantic space of the following form:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; `se_image(img_features) = img_features M + b`\n",
    "\n",
    "where `M` is a parameter of shape `(512, 50)` and `b` is a parameter of shape `(1, 50)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training\n",
    "\n",
    "To find a good values for parameters `M` and `b`, we'll create a training set containing triples of the form: \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; `(text, good_image, bad_image)`\n",
    "\n",
    "We want the similarity in semantic space between `text` and `good_image` to be greater than the similarity between `text` and `bad_image`, i.e.,\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; `sim(se_text(text), se_image(good_image)) > sim(se_text(text), se_image(bad_image))`\n",
    "\n",
    "To encourage this relationship to be true for a triple, we'll use a loss function called the \"margin ranking loss\" (e.g., `mygrad.nnet.margin_ranking_loss`). This loss penalizes when the ordering of the values is wrong, and stops penalizing once the order is right \"enough\" (determined by the desired margin). The reasoning is that once the ordering between values is right, we don't need to waste effort trying to make it even more right.\n",
    "\n",
    "The margin ranking loss is defined as:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; `loss(x1, x2, y, margin) = maximum(0, margin - y * (x1 - x2)))`\n",
    "\n",
    "where `y = 1` means `x1` should be higher than `x2` and `y = 0` means `x2` should be ranked higher than `x1`.\n",
    "\n",
    "If we let\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; `sim_to_good = sim(se_text(text), se_image(good_image))`\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; `sim_to_bad = sim(se_text(text), se_image(bad_image))`\n",
    "\n",
    "then the loss for a single triple would be:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; `loss(sim_to_good, sim_to_bad, 1, margin`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Enhanced training set\n",
    "\n",
    "Researchers have found that generating totally random triples for training doesn't usually result in the best performance. In the context of our image search project, notice that picking a random image and one of its captions for `text` and `good_image`, and then picking a totally random other image for `bad_image` will often result in an \"easy\" triple.\n",
    "\n",
    "For example, `good_image` and `text` might be \"a dog catching a frisbee\", while `bad_image` is a picture of a pizza. During training, the model will learn to make these kinds of easy distinctions, but might not be able to make harder ones. For example, it might have trouble properly ranking images of dogs with frisbees verses dogs swimming (in response to queries like \"dog with frisbee\" or \"dog in water\") because somewhat similar images won't be generated too often when constructing the training set totally randomly.\n",
    "\n",
    "Here's a simple approach for generating more \"challenging\" triples for the training set. Once a `good_image` and `text` (one of the good image's captions) are chosen, randomly sample a small set of potential bad images. Then pick the bad image that has a caption that's most similar to `text` (in terms of cosine similarity between the semantic embeddings of the captions). This should result in better query performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Dataset\n",
    "\n",
    "We'll be using the Microsoft COCO dataset. From the website (http://cocodataset.org/):\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"COCO is a large-scale object detection, segmentation, and captioning dataset.\"\n",
    "\n",
    "The file `captions_train2014.json` contains all the COCO image metadata and annotations (captions) for the official training set from 2014.\n",
    "\n",
    "Use `json.load()` to convert this file to a dictionary with keys:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `['info', 'images', 'licenses', 'annotations']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cogworks_data.language import get_data_path\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Load COCO metadata\n",
    "filename = get_data_path(\"captions_train2014.json\")\n",
    "with Path(filename).open() as f:\n",
    "    coco_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Image Search -- Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Goal\n",
    "\n",
    "The goal of this week's capstone project is to combine techniques from computer vision and natural language processing (NLP) to build a system that allows users to query for images using keywords. For example, a search for \"**pizza**\" would return results such as:\n",
    "\n",
    "<img src=\"pizza_crop.png\" width=\"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Approach\n",
    "\n",
    "How are we going to achieve this? We're going to map (or \"embed\") both textual captions and images to a common \"semantic\" space. Let's assume this semantic space has dimension 50. Let\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; `se_text()` be a function that maps a piece of text to its semantic embedding, and\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; `se_image()` be a function that maps an image to its semantic embedding.\n",
    "\n",
    "We want these mappings to have the property that a caption and image with similar meanings will map to semantic embeddings that are near each other in the semantic space. (This concept should sound familiar from our earlier work with word embeddings...) For example, we would like `se_text('pizza')` to be close to `se_image(pizza_slice_img)`. \n",
    "\n",
    "We'll use cosine similarity to measure the similarity between two vectors: \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{sim}(\\vec{x}, \\vec{y}) = \\cos{\\theta} = \\frac{\\vec{x} \\cdot \\vec{y}}{\\lVert \\vec{x} \\rVert \\lVert \\vec{y} \\rVert}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lVert \\vec{x} \\rVert = \\sqrt{x_0^2 + x_1^2 + ...}$ is the magnitude of $\\vec{x}$ and $\\vec{x} \\cdot \\vec{y}$ is the *dot product* of the two vectors. Note that if both vectors are already normalized to have unit length, then cosine similarity is equivalent to the dot product between the two vectors.\n",
    "\n",
    "Once we have `se_text()` and `se_image()`, we can build our image search system:\n",
    "- **Preprocessing**: Create an image database by running a collection of images through `se_image()` and saving their semantic embeddings.\n",
    "- **Search by text**: To process a text query, compute the cosine similarity between the semantic embedding of the query, `se_text(query)`, with all the semantic embeddings in the image database and return the ones with the highest scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Text Embedding\n",
    "\n",
    "We'll embed text by essentially averaging the word embeddings (e.g., GloVe embeddings of dimension 50) for all words in the string.\n",
    "\n",
    "We'll weight each word by the inverse document frequency (IDF) of the word (computed across all captions in the training data) in order to down-weight common words.\n",
    "\n",
    "We'll also normalize the weighted sum to have unit length so that similarities can be computed with the dot product.\n",
    "\n",
    "Note that there are no trainable parameters here. You can load in the GloVe embeddings as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Image Embedding\n",
    "### 4.1 Image Features\n",
    "\n",
    "To bootstrap our image embedding capability, we're going to make use of a pre-trained computer vision model that was trained to do image classification on the ImageNet dataset. (For more info on ImageNet, see: http://www.image-net.org/).\n",
    "\n",
    "In particular, we're going to use the ResNet-18 model (implemented in PyTorch). ResNet-18 is a type of convolutional neural network (CNN). The last layer of ResNet-18 is a fully connected layer (\"dense\" layer in MyNN terminology; \"Linear\" layer in PyTorch terminology) that projects the final 512-dimensional \"abstract features\" to 1000-dimensional scores used for computing probabilities that an input image is one of the 1000 possible ImageNet classes. We're going to use these 512-dimensional abstract features (which have distilled useful properties/characteristics/aspects of the image) as a starting point for our image embedder.\n",
    "\n",
    "You can imagine that we have a function called `extract_features_resnet18(image)` that takes an image (e.g., in PIL format), runs it through the ResNet-18 model, and then returns a NumPy array of shape (1, 512). You don't have to write this function yourself, though! We've already pre-extracted these 512-dimensional image features for images in the COCO dataset (described below).\n",
    "\n",
    "The file `resnet18_features.pkl` contains a dictionary that maps image ids (from the COCO dataset) to extracted image features, imported below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Semantic Embedding\n",
    "\n",
    "We will learn a mapping from the 512-dimensional image feature space to the common 50-dimensional semantic space of the following form:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; `se_image(img_features) = img_features M + b`\n",
    "\n",
    "where `M` is a parameter of shape `(512, 50)` and `b` is a parameter of shape `(1, 50)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Enhanced training set\n",
    "\n",
    "Researchers have found that generating totally random triples for training doesn't usually result in the best performance. In the context of our image search project, notice that picking a random image and one of its captions for `text` and `good_image`, and then picking a totally random other image for `bad_image` will often result in an \"easy\" triple.\n",
    "\n",
    "For example, `good_image` and `text` might be \"a dog catching a frisbee\", while `bad_image` is a picture of a pizza. During training, the model will learn to make these kinds of easy distinctions, but might not be able to make harder ones. For example, it might have trouble properly ranking images of dogs with frisbees verses dogs swimming (in response to queries like \"dog with frisbee\" or \"dog in water\") because somewhat similar images won't be generated too often when constructing the training set totally randomly.\n",
    "\n",
    "Here's a simple approach for generating more \"challenging\" triples for the training set. Once a `good_image` and `text` (one of the good image's captions) are chosen, randomly sample a small set of potential bad images. Then pick the bad image that has a caption that's most similar to `text` (in terms of cosine similarity between the semantic embeddings of the captions). This should result in better query performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cogworks_data.language import get_data_path\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Load COCO metadata\n",
    "filename = get_data_path(\"captions_train2014.json\")\n",
    "with Path(filename).open() as f:\n",
    "    coco_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Tasks for Team\n",
    "\n",
    "These tasks are the minimum set of things that need to be completed as part of the capstone project. You should coordinate with your team about how to divide them up. (Note: Some might naturally be combined.)\n",
    "\n",
    "* create capability to embed text (captions and queries)\n",
    "* create training and validation sets of triples\n",
    "* create function to compute loss, accuracy (in terms of triples correct)\n",
    "* create MyNN model for embedding images\n",
    "* train model\n",
    " * embed caption\n",
    " * embed good image\n",
    " * embed bad image\n",
    " * compute similarities from caption to good and caption to bad\n",
    " * compute loss with margin ranking loss\n",
    " * take optimization step\n",
    "* create image \"database\" by mapping whole set of image features to semantic features with trained model\n",
    "* create function to query database and return top k images\n",
    "* create function to display set of images (given image ids)\n",
    " * note that the image metadata (contained in `captions_train2014.json`) includes a property called \"coco_url\" that can be used download a particular image on demand for display\n",
    " * maybe display their captions, too (for debugging)\n",
    "* create function that finds top k similar images to a query image\n",
    " * maybe give option for doing similarity search in either semantic space or original image feature space"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
